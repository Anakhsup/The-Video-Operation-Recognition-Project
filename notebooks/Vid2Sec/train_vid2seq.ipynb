{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from moviepy.editor import VideoFileClip\n",
    "from tqdm.autonotebook import tqdm\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RkVdgMkEOQTJ",
   "metadata": {
    "id": "RkVdgMkEOQTJ"
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1yjaLqF5ODzm",
   "metadata": {
    "id": "1yjaLqF5ODzm"
   },
   "outputs": [],
   "source": [
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"\n",
    "    3x3x3 convolution with padding.\n",
    "\n",
    "    Args:\n",
    "        in_planes (int): Number of input channels.\n",
    "        out_planes (int): Number of output channels.\n",
    "        stride (int): Stride of the convolution.\n",
    "\n",
    "    Returns:\n",
    "        nn.Conv3d: Convolutional layer.\n",
    "    \"\"\"\n",
    "    return nn.Conv3d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=1,\n",
    "        bias=False)\n",
    "\n",
    "def downsample_basic_block(x, planes, stride):\n",
    "    \"\"\"\n",
    "    Downsample the input tensor using average pooling and zero-padding.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        planes (int): Number of output channels.\n",
    "        stride (int): Stride of the pooling.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Downsampled tensor.\n",
    "    \"\"\"\n",
    "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "    zero_pads = torch.Tensor(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4)).zero_()\n",
    "    if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "        zero_pads = zero_pads.cuda()\n",
    "    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n",
    "\n",
    "    return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Bottleneck block for ResNet.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, head_conv=1):\n",
    "        \"\"\"\n",
    "        Initializes the Bottleneck block.\n",
    "\n",
    "        Args:\n",
    "            inplanes (int): Number of input channels.\n",
    "            planes (int): Number of output channels.\n",
    "            stride (int): Stride of the convolution.\n",
    "            downsample (nn.Module, optional): Downsampling layer. Defaults to None.\n",
    "            head_conv (int, optional): Type of head convolution. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if head_conv == 1:\n",
    "            self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm3d(planes)\n",
    "        elif head_conv == 3:\n",
    "            self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=(3, 1, 1), bias=False, padding=(1, 0, 0))\n",
    "            self.bn1 = nn.BatchNorm3d(planes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported head_conv!\")\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            planes, planes, kernel_size=(1, 3, 3), stride=(1, stride, stride), padding=(0, 1, 1), bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Bottleneck block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def get_fine_tuning_parameters(model, ft_begin_index):\n",
    "    \"\"\"\n",
    "    Get the parameters for fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to fine-tune.\n",
    "        ft_begin_index (int): Index of the first layer to fine-tune.\n",
    "\n",
    "    Returns:\n",
    "        list: List of parameters.\n",
    "    \"\"\"\n",
    "    if ft_begin_index == 0:\n",
    "        return model.parameters()\n",
    "\n",
    "    ft_module_names = []\n",
    "    for i in range(ft_begin_index, 5):\n",
    "        ft_module_names.append('layer{}'.format(i))\n",
    "    ft_module_names.append('fc')\n",
    "\n",
    "    parameters = []\n",
    "    for k, v in model.named_parameters():\n",
    "        for ft_module in ft_module_names:\n",
    "            if ft_module in k:\n",
    "                parameters.append({'params': v})\n",
    "                break\n",
    "        else:\n",
    "            parameters.append({'params': v, 'lr': 0.0})\n",
    "\n",
    "    return parameters\n",
    "\n",
    "class SlowFast(nn.Module):\n",
    "    \"\"\"\n",
    "    SlowFast network for video recognition.\n",
    "    \"\"\"\n",
    "    def __init__(self, block=Bottleneck, layers=[3, 4, 6, 3], class_num=27, shortcut_type='B', dropout=0.5,\n",
    "                 alpha=8, beta=0.125):\n",
    "        \"\"\"\n",
    "        Initializes the SlowFast network.\n",
    "\n",
    "        Args:\n",
    "            block (nn.Module, optional): Bottleneck block. Defaults to Bottleneck.\n",
    "            layers (list, optional): Number of layers in each block. Defaults to [3, 4, 6, 3].\n",
    "            class_num (int, optional): Number of classes. Defaults to 27.\n",
    "            shortcut_type (str, optional): Type of shortcut connection. Defaults to 'B'.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.5.\n",
    "            alpha (int, optional): Temporal stride for slow path. Defaults to 8.\n",
    "            beta (float, optional): Channel ratio between fast and slow path. Defaults to 0.125.\n",
    "        \"\"\"\n",
    "        super(SlowFast, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        self.fast_inplanes = int(64 * beta)\n",
    "        fast_inplanes = self.fast_inplanes\n",
    "        self.fast_conv1 = nn.Conv3d(3, fast_inplanes, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3),\n",
    "                                    bias=False)\n",
    "        self.fast_bn1 = nn.BatchNorm3d(8)\n",
    "        self.fast_relu = nn.ReLU(inplace=True)\n",
    "        self.fast_maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n",
    "        self.fast_res1 = self._make_layer_fast(block, 8, layers[0], shortcut_type, head_conv=3)\n",
    "        self.fast_res2 = self._make_layer_fast(\n",
    "            block, 16, layers[1], shortcut_type, stride=2, head_conv=3)\n",
    "        self.fast_res3 = self._make_layer_fast(\n",
    "            block, 32, layers[2], shortcut_type, stride=2, head_conv=3)\n",
    "        self.fast_res4 = self._make_layer_fast(\n",
    "            block, 64, layers[3], shortcut_type, stride=2, head_conv=3)\n",
    "\n",
    "        self.slow_inplanes = 64\n",
    "        slow_inplanes = self.slow_inplanes\n",
    "        self.slow_conv1 = nn.Conv3d(3, slow_inplanes, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3),\n",
    "                                    bias=False)\n",
    "        self.slow_bn1 = nn.BatchNorm3d(64)\n",
    "        self.slow_relu = nn.ReLU(inplace=True)\n",
    "        self.slow_maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n",
    "        self.slow_res1 = self._make_layer_slow(block, 64, layers[0], shortcut_type, head_conv=1)\n",
    "        self.slow_res2 = self._make_layer_slow(\n",
    "            block, 128, layers[1], shortcut_type, stride=2, head_conv=1)\n",
    "        self.slow_res3 = self._make_layer_slow(\n",
    "            block, 256, layers[2], shortcut_type, stride=2, head_conv=1)\n",
    "        self.slow_res4 = self._make_layer_slow(\n",
    "            block, 512, layers[3], shortcut_type, stride=2, head_conv=1)\n",
    "\n",
    "        self.Tconv1 = nn.Conv3d(8, 16, kernel_size=(5, 1, 1), stride=(alpha, 1, 1), padding=(2, 0, 0), bias=False)\n",
    "        self.Tconv2 = nn.Conv3d(32, 64, kernel_size=(5, 1, 1), stride=(alpha, 1, 1), padding=(2, 0, 0), bias=False)\n",
    "        self.Tconv3 = nn.Conv3d(64, 128, kernel_size=(5, 1, 1), stride=(alpha, 1, 1), padding=(2, 0, 0), bias=False)\n",
    "        self.Tconv4 = nn.Conv3d(128, 256, kernel_size=(5, 1, 1), stride=(alpha, 1, 1), padding=(2, 0, 0), bias=False)\n",
    "\n",
    "        self.dp = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.fast_inplanes + self.slow_inplanes, class_num)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass of the SlowFast network.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        fast, Tc = self.FastPath(input[:, :, ::2, :, :])\n",
    "        slow = self.SlowPath(input[:, :, ::16, :, :], Tc)\n",
    "        x = torch.cat([slow, fast], dim=1)\n",
    "        x = self.dp(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def SlowPath(self, input, Tc):\n",
    "        \"\"\"\n",
    "        Forward pass of the slow path.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "            Tc (list): List of temporal convolutional features from the fast path.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        x = self.slow_conv1(input)\n",
    "        x = self.slow_bn1(x)\n",
    "        x = self.slow_relu(x)\n",
    "        x = self.slow_maxpool(x)\n",
    "        x = torch.cat([x, Tc[0]], dim=1)\n",
    "        x = self.slow_res1(x)\n",
    "        x = torch.cat([x, Tc[1]], dim=1)\n",
    "        x = self.slow_res2(x)\n",
    "        x = torch.cat([x, Tc[2]], dim=1)\n",
    "        x = self.slow_res3(x)\n",
    "        x = torch.cat([x, Tc[3]], dim=1)\n",
    "        x = self.slow_res4(x)\n",
    "        x = nn.AdaptiveAvgPool3d(1)(x)\n",
    "        x = x.view(-1, x.size(1))\n",
    "        return x\n",
    "\n",
    "    def FastPath(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass of the fast path.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Output tensor and list of temporal convolutional features.\n",
    "        \"\"\"\n",
    "        x = self.fast_conv1(input)\n",
    "        x = self.fast_bn1(x)\n",
    "        x = self.fast_relu(x)\n",
    "        x = self.fast_maxpool(x)\n",
    "        Tc1 = self.Tconv1(x)\n",
    "        x = self.fast_res1(x)\n",
    "        Tc2 = self.Tconv2(x)\n",
    "        x = self.fast_res2(x)\n",
    "        Tc3 = self.Tconv3(x)\n",
    "        x = self.fast_res3(x)\n",
    "        Tc4 = self.Tconv4(x)\n",
    "        x = self.fast_res4(x)\n",
    "        x = nn.AdaptiveAvgPool3d(1)(x)\n",
    "        x = x.view(-1, x.size(1))\n",
    "        return x, [Tc1, Tc2, Tc3, Tc4]\n",
    "\n",
    "    def _make_layer_fast(self, block, planes, blocks, shortcut_type, stride=1, head_conv=1):\n",
    "        \"\"\"\n",
    "        Make a fast layer.\n",
    "\n",
    "        Args:\n",
    "            block (nn.Module): Bottleneck block.\n",
    "            planes (int): Number of output channels.\n",
    "            blocks (int): Number of blocks in the layer.\n",
    "            shortcut_type (str): Type of shortcut connection.\n",
    "            stride (int, optional): Stride of the convolution. Defaults to 1.\n",
    "            head_conv (int, optional): Type of head convolution. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: Sequential layer.\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if stride != 1 or self.fast_inplanes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(\n",
    "                    downsample_basic_block,\n",
    "                    planes=planes * block.expansion,\n",
    "                    stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(\n",
    "                        self.fast_inplanes,\n",
    "                        planes * block.expansion,\n",
    "                        kernel_size=1,\n",
    "                        stride=(1, stride, stride),\n",
    "                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.fast_inplanes, planes, stride, downsample, head_conv=head_conv))\n",
    "        self.fast_inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.fast_inplanes, planes, head_conv=head_conv))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_layer_slow(self, block, planes, blocks, shortcut_type, stride=1, head_conv=1):\n",
    "        \"\"\"\n",
    "        Make a slow layer.\n",
    "\n",
    "        Args:\n",
    "            block (nn.Module): Bottleneck block.\n",
    "            planes (int): Number of output channels.\n",
    "            blocks (int): Number of blocks in the layer.\n",
    "            shortcut_type (str): Type of shortcut connection.\n",
    "            stride (int, optional): Stride of the convolution. Defaults to 1.\n",
    "            head_conv (int, optional): Type of head convolution. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: Sequential layer.\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if stride != 1 or self.slow_inplanes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(\n",
    "                    downsample_basic_block,\n",
    "                    planes=planes * block.expansion,\n",
    "                    stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(\n",
    "                        self.slow_inplanes + self.slow_inplanes // self.alpha * 2,\n",
    "                        planes * block.expansion,\n",
    "                        kernel_size=1,\n",
    "                        stride=(1, stride, stride),\n",
    "                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.slow_inplanes + self.slow_inplanes // self.alpha * 2, planes, stride, downsample,\n",
    "                            head_conv=head_conv))\n",
    "        self.slow_inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.slow_inplanes, planes, head_conv=head_conv))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-50 model.\n",
    "    \"\"\"\n",
    "    model = SlowFast(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-101 model.\n",
    "    \"\"\"\n",
    "    model = SlowFast(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-152 model.\n",
    "    \"\"\"\n",
    "    model = SlowFast(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet200(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-200 model.\n",
    "    \"\"\"\n",
    "    model = SlowFast(Bottleneck, [3, 24, 36, 3], **kwargs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MmrPHRFPOUxy",
   "metadata": {
    "id": "MmrPHRFPOUxy"
   },
   "source": [
    "Clr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9mrP5Gt6OVsi",
   "metadata": {
    "id": "9mrP5Gt6OVsi"
   },
   "outputs": [],
   "source": [
    "class OneCycle(object):\n",
    "    \"\"\"Implements the 1cycle learning rate/momentum scheduler.\n",
    "    \n",
    "    Cycles LR between max_lr/div and max_lr, and momentum between high/low values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nb, max_lr, momentum_vals=(0.95, 0.85), prcnt=10, div=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            nb: Total number of iterations\n",
    "            max_lr: Maximum learning rate\n",
    "            momentum_vals: Tuple of (high_momentum, low_momentum)\n",
    "            prcnt: Percentage of iterations for annealing (default: 10)\n",
    "            div: Division factor for minimum LR (default: 10)\n",
    "        \"\"\"\n",
    "        self.nb = nb\n",
    "        self.div = div\n",
    "        self.step_len = int(self.nb * (1 - prcnt/100)/2)\n",
    "        self.high_lr = max_lr\n",
    "        self.low_mom = momentum_vals[1]\n",
    "        self.high_mom = momentum_vals[0]\n",
    "        self.prcnt = prcnt\n",
    "        self.iteration = 0\n",
    "        self.lrs = []\n",
    "        self.moms = []\n",
    "\n",
    "    def calc(self):\n",
    "        \"\"\"Returns current (learning_rate, momentum) pair.\"\"\"\n",
    "        self.iteration += 1\n",
    "        lr = self.calc_lr()\n",
    "        mom = self.calc_mom()\n",
    "        return (lr, mom)\n",
    "\n",
    "    def calc_lr(self):\n",
    "        \"\"\"Calculates current learning rate based on iteration progress.\"\"\"\n",
    "        if self.iteration == self.nb:\n",
    "            self.iteration = 0\n",
    "            self.lrs.append(self.high_lr/self.div)\n",
    "            return self.high_lr/self.div\n",
    "        if self.iteration > 2 * self.step_len:\n",
    "            ratio = (self.iteration - 2 * self.step_len) / (self.nb - 2 * self.step_len)\n",
    "            lr = self.high_lr * (1 - 0.99 * ratio)/self.div\n",
    "        elif self.iteration > self.step_len:\n",
    "            ratio = 1 - (self.iteration - self.step_len)/self.step_len\n",
    "            lr = self.high_lr * (1 + ratio * (self.div - 1)) / self.div\n",
    "        else:\n",
    "            ratio = self.iteration/self.step_len\n",
    "            lr = self.high_lr * (1 + ratio * (self.div - 1)) / self.div\n",
    "        self.lrs.append(lr)\n",
    "        return lr\n",
    "\n",
    "    def calc_mom(self):\n",
    "        \"\"\"Calculates current momentum based on iteration progress.\"\"\"\n",
    "        if self.iteration == self.nb:\n",
    "            self.iteration = 0\n",
    "            self.moms.append(self.high_mom)\n",
    "            return self.high_mom\n",
    "        if self.iteration > 2 * self.step_len:\n",
    "            mom = self.high_mom\n",
    "        elif self.iteration > self.step_len:\n",
    "            ratio = (self.iteration - self.step_len)/self.step_len\n",
    "            mom = self.low_mom + ratio * (self.high_mom - self.low_mom)\n",
    "        else:\n",
    "            ratio = self.iteration/self.step_len\n",
    "            mom = self.high_mom - ratio * (self.high_mom - self.low_mom)\n",
    "        self.moms.append(mom)\n",
    "        return mom\n",
    "\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    \"\"\"Updates learning rate for all parameter groups in optimizer.\"\"\"\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "\n",
    "def update_mom(optimizer, mom):\n",
    "    \"\"\"Updates momentum for all parameter groups in optimizer.\"\"\"\n",
    "    for g in optimizer.param_groups:\n",
    "        g['momentum'] = mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "r6BlG4IhKyAF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6BlG4IhKyAF",
    "outputId": "22b7eee8-5925-4447-fc7e-56fecc9295b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gas_analyzer', 'syringing', 'inspection', 'measure', 'pipe_work', 'HRW_work', 'pipe_up', 'pipe_down', 'cleaning', 'open_mouth', 'gaskets', 'spider_landing', 'unscrewing PCP', 'remove_PCP', 'unscrewing_PCP', 'crane_lowering', 'crane_lifting', 'PCP_roll', 'cross_rotation']\n",
      "Decoder: {'gas_analyzer': 0, 'syringing': 1, 'inspection': 2, 'measure': 3, 'pipe_work': 4, 'HRW_work': 5, 'pipe_up': 6, 'pipe_down': 7, 'cleaning': 8, 'open_mouth': 9, 'gaskets': 10, 'spider_landing': 11, 'unscrewing PCP': 12, 'remove_PCP': 13, 'unscrewing_PCP': 14, 'crane_lowering': 15, 'crane_lifting': 16, 'PCP_roll': 17, 'cross_rotation': 18}\n",
      "Encoder: {0: 'gas_analyzer', 1: 'syringing', 2: 'inspection', 3: 'measure', 4: 'pipe_work', 5: 'HRW_work', 6: 'pipe_up', 7: 'pipe_down', 8: 'cleaning', 9: 'open_mouth', 10: 'gaskets', 11: 'spider_landing', 12: 'unscrewing PCP', 13: 'remove_PCP', 14: 'unscrewing_PCP', 15: 'crane_lowering', 16: 'crane_lifting', 17: 'PCP_roll', 18: 'cross_rotation'}\n"
     ]
    }
   ],
   "source": [
    "data_path = '/root/tatneft/datasets/violations_dataset/cuts1_labels.txt'\n",
    "\n",
    "# Read class labels from file and store them in a list\n",
    "with open(data_path, 'r') as file:\n",
    "    classes = [line.strip() for line in file.readlines()]\n",
    "\n",
    "print(classes)\n",
    "\n",
    "# Create a decoder dictionary that maps class labels to their index\n",
    "decoder = {classes[i]: i for i in range(len(classes))}\n",
    "\n",
    "# Create an encoder dictionary that maps indices to class labels\n",
    "encoder = {i: classes[i] for i in range(len(classes))}\n",
    "\n",
    "print(\"Decoder:\", decoder)\n",
    "print(\"Encoder:\", encoder)\n",
    "\n",
    "# List to store video paths and class labels\n",
    "id_list = []\n",
    "\n",
    "# Path to the annotation file containing video paths and corresponding class labels\n",
    "annotations_path = '/root/tatneft/datasets/violations_dataset/cuts1_train.txt'\n",
    "\n",
    "# Read the annotation file and store video paths with class labels in id_list\n",
    "with open(annotations_path, 'r') as file:\n",
    "    for line in file:\n",
    "        video_path, class_label = line.strip().split()\n",
    "        id_list.append((video_path, int(class_label)))\n",
    "\n",
    "# Print the total number of entries and the first five samples\n",
    "print(len(id_list))\n",
    "print(id_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89b1b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class video_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading video sequences with fixed frame length.\n",
    "    \"\"\"\n",
    "    def __init__(self, frame_list, sequence_length=16, transform=None, im_size=256):\n",
    "        self.frame_list = frame_list\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "        self.im_size = im_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.frame_list[idx]\n",
    "        path = '/root/tatneft/datasets/violations_dataset/cuts1/' + path\n",
    "\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Failed to open video file: {path}\")\n",
    "\n",
    "        frames = []\n",
    "        while len(frames) < self.sequence_length:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        while len(frames) < self.sequence_length:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "\n",
    "        seq_image = torch.stack(frames)  # Shape: (sequence_length, channels, height, width)\n",
    "        seq_image = seq_image.permute(1, 0, 2, 3)  # Shape: (channels, sequence_length, height, width)\n",
    "\n",
    "        return seq_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f436a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6f436a2",
    "outputId": "4c023790-c064-4e80-ba8c-155aac88b86a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'val'])\n"
     ]
    }
   ],
   "source": [
    "# Image parameters\n",
    "im_size = 128\n",
    "mean = [0.4889, 0.4887, 0.4891]\n",
    "std = [0.2074, 0.2074, 0.2074]\n",
    "\n",
    "# Define transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((im_size, im_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load training data\n",
    "train_data = video_dataset(id, sequence_length=10, transform=train_transforms)\n",
    "train_loader = DataLoader(train_data, batch_size=8, num_workers=1, shuffle=True)\n",
    "\n",
    "# Load validation data\n",
    "val_path = '/root/tatneft/datasets/violations_dataset/cuts1_val.txt'\n",
    "val_id = []\n",
    "with open(val_path, 'r') as file:\n",
    "    for line in file:\n",
    "        video_path, class_label = line.strip().split()\n",
    "        val_id.append((video_path, int(class_label)))\n",
    "\n",
    "val_data = video_dataset(val_id, sequence_length=10, transform=train_transforms)\n",
    "val_loader = DataLoader(val_data, batch_size=8, num_workers=1, shuffle=False)\n",
    "\n",
    "# Create dataloaders dictionary\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "# Print available keys\n",
    "print(dataloaders.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db7609be",
   "metadata": {
    "id": "db7609be"
   },
   "outputs": [],
   "source": [
    "model = resnet200(class_num=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faada049",
   "metadata": {
    "id": "faada049"
   },
   "outputs": [],
   "source": [
    "\"\"\"Learnig parameters\"\"\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "cls_criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum = 0.9,weight_decay = 1e-4)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "onecyc = OneCycle(len(train_loader)*num_epochs,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c25137be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:43,  2.15s/it]\n",
      "10it [00:20,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.6855, Val_loss: 65.3912, Precision: 0.1157, Recall: 0.2785, F1: 0.1612\n",
      "\n",
      "--- Epoch 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:37,  2.04s/it]\n",
      "10it [00:19,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.6410, Val_loss: 3.8591, Precision: 0.1792, Recall: 0.2278, F1: 0.1901\n",
      "\n",
      "--- Epoch 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:36,  2.02s/it]\n",
      "10it [00:19,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.6023, Val_loss: 2.4220, Precision: 0.1760, Recall: 0.3038, F1: 0.2116\n",
      "\n",
      "--- Epoch 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:39,  2.07s/it]\n",
      "10it [00:20,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.6003, Val_loss: 2.1498, Precision: 0.2019, Recall: 0.2911, F1: 0.2326\n",
      "\n",
      "--- Epoch 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:36,  2.02s/it]\n",
      "10it [00:19,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.5035, Val_loss: 2.0964, Precision: 0.2764, Recall: 0.4557, F1: 0.3300\n",
      "\n",
      "--- Epoch 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:39,  2.07s/it]\n",
      "10it [00:19,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.4698, Val_loss: 2.0740, Precision: 0.2466, Recall: 0.3418, F1: 0.2723\n",
      "\n",
      "--- Epoch 7 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:54,  2.38s/it]\n",
      "10it [00:20,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.4280, Val_loss: 1.9892, Precision: 0.2316, Recall: 0.3038, F1: 0.2426\n",
      "\n",
      "--- Epoch 8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:37,  2.03s/it]\n",
      "10it [00:20,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.4173, Val_loss: 3.6172, Precision: 0.1321, Recall: 0.2785, F1: 0.1739\n",
      "\n",
      "--- Epoch 9 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:39,  2.08s/it]\n",
      "10it [00:20,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.4288, Val_loss: 2.0961, Precision: 0.2880, Recall: 0.4430, F1: 0.3292\n",
      "\n",
      "--- Epoch 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:37,  2.03s/it]\n",
      "10it [00:20,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.1298, Val_loss: 1.9297, Precision: 0.3571, Recall: 0.3165, F1: 0.2943\n",
      "\n",
      "--- Epoch 11 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:36,  2.02s/it]\n",
      "10it [00:19,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.1147, Val_loss: 1.7433, Precision: 0.3899, Recall: 0.4304, F1: 0.3913\n",
      "\n",
      "--- Epoch 12 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:38,  2.05s/it]\n",
      "10it [00:20,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.0902, Val_loss: 1.7476, Precision: 0.4574, Recall: 0.3544, F1: 0.3323\n",
      "\n",
      "--- Epoch 13 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:37,  2.03s/it]\n",
      "10it [00:19,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.0066, Val_loss: 1.4516, Precision: 0.3391, Recall: 0.4177, F1: 0.3637\n",
      "\n",
      "--- Epoch 14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:40,  2.09s/it]\n",
      "10it [00:20,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.7505, Val_loss: 1.6653, Precision: 0.3179, Recall: 0.3671, F1: 0.3296\n",
      "\n",
      "--- Epoch 15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:38,  2.05s/it]\n",
      "10it [00:20,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.8837, Val_loss: 1.4327, Precision: 0.4706, Recall: 0.5443, F1: 0.4846\n",
      "\n",
      "--- Epoch 16 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:38,  2.05s/it]\n",
      "10it [00:20,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.7939, Val_loss: 1.5954, Precision: 0.3680, Recall: 0.4177, F1: 0.3639\n",
      "\n",
      "--- Epoch 17 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:37,  2.02s/it]\n",
      "10it [00:20,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.7613, Val_loss: 1.1609, Precision: 0.3662, Recall: 0.5949, F1: 0.4510\n",
      "\n",
      "--- Epoch 18 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:37,  2.03s/it]\n",
      "10it [00:19,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.4146, Val_loss: 1.3356, Precision: 0.4345, Recall: 0.4557, F1: 0.3942\n",
      "\n",
      "--- Epoch 19 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:34,  1.97s/it]\n",
      "10it [00:19,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.5187, Val_loss: 1.0230, Precision: 0.5884, Recall: 0.6962, F1: 0.6200\n",
      "\n",
      "--- Epoch 20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:35,  2.00s/it]\n",
      "10it [00:19,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.3856, Val_loss: 1.0356, Precision: 0.5970, Recall: 0.6709, F1: 0.5832\n",
      "Best F1-score: 0.6200 in epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create directory for saving model weights\n",
    "os.makedirs('/root/akhsup/weights', exist_ok=True)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Initialize training variables\n",
    "iteration = 0\n",
    "train_losses, val_losses = [], []\n",
    "precisions, recalls, f1_scores = [], [], []\n",
    "train_loss, val_loss = 0.0, 0.0\n",
    "best_f1 = 0.0\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n--- Epoch {epoch+1} ---\")\n",
    "    \n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        all_labels, all_preds = [], []\n",
    "\n",
    "        for batch_i, (X, y) in tqdm(enumerate(dataloaders[phase])):\n",
    "            image_sequences = X.to(device)\n",
    "            labels = y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                predictions = model(image_sequences)\n",
    "                loss = cls_criterion(predictions, labels)\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * image_sequences.size(0)\n",
    "            _, preds = torch.max(predictions, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            # Update learning rate and momentum for training phase\n",
    "            if phase == 'train':\n",
    "                lr, mom = onecyc.calc()\n",
    "                update_lr(optimizer, lr)\n",
    "                update_mom(optimizer, mom)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Compute average loss and metrics\n",
    "        epoch_loss /= len(dataloaders[phase].dataset)\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        \n",
    "        if phase == 'train':\n",
    "            train_losses.append(epoch_loss)\n",
    "            train_loss = epoch_loss\n",
    "        else:\n",
    "            val_losses.append(epoch_loss)\n",
    "            val_loss = epoch_loss\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "            # Save best model based on F1-score\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), f'/root/akhsup/weights/best_model_f1_{best_epoch}.pth')\n",
    "    \n",
    "    print(f\"Train_loss: {train_loss:.4f}, Val_loss: {val_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "print(f\"Best F1-score: {best_f1:.4f} in epoch {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "526204b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss (try 1)')\n",
    "plt.legend()\n",
    "plt.savefig('/root/akhsup/weights/loss_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ae790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, num_classes=19):\n",
    "    \"\"\"\n",
    "    Load the pretrained model from the specified path.\n",
    "    \"\"\"\n",
    "    model = resnet200(class_num=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_video(video_path, sequence_length=10, im_size=128):\n",
    "    \"\"\"\n",
    "    Preprocess the video into a tensor that can be fed into the model.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((im_size, im_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4889, 0.4887, 0.4891], std=[0.2074, 0.2074, 0.2074])\n",
    "    ])\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while len(frames) < sequence_length:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = transform(frame) \n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        raise ValueError(f\"Ошибка: Видео '{video_path}' не содержит кадров или файл повреждён.\")\n",
    "\n",
    "    # Pad the frames to ensure the sequence length is met\n",
    "    while len(frames) < sequence_length:\n",
    "        frames.append(torch.zeros_like(frames[-1]))\n",
    "\n",
    "    # Convert frames to a tensor and permute to (channels, sequence_length, height, width)\n",
    "    video_tensor = torch.stack(frames)  # (sequence_length, channels, height, width)\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)  # (channels, sequence_length, height, width)\n",
    "    \n",
    "    return video_tensor.unsqueeze(0)  # Add batch dimension: (1, channels, sequence_length, height, width)\n",
    "\n",
    "def predict_video(model, video_tensor, device):\n",
    "    \"\"\"\n",
    "    Perform prediction on a video tensor using the given model.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        video_tensor = video_tensor.to(device)\n",
    "        outputs = model(video_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "def test_video(model_path, video_path, encoder, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Test the model on a video, returning the predicted class.\n",
    "    \"\"\"\n",
    "    model = load_model(model_path)\n",
    "    model = model.to(device)\n",
    "\n",
    "    video_tensor = preprocess_video(video_path)\n",
    "\n",
    "    predicted_class = predict_video(model, video_tensor, device)\n",
    "    \n",
    "    # Map the predicted class index to its label\n",
    "    predicted_label = encoder.get(predicted_class, \"Unknown class\")\n",
    "    \n",
    "    # You can print the label or the class index for debugging or logging\n",
    "    print(f\"Predicted class: {predicted_label} (index {predicted_class})\")\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     model_path = \"/path/to/model.pth\"\n",
    "#     video_path = \"/path/to/video.mp4\"\n",
    "#     encoder = {0: \"Class1\", 1: \"Class2\", 2: \"Class3\", ...}  # Define your encoder\n",
    "#     predicted_class = test_video(model_path, video_path, encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d05a3e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_762807/2322475451.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of Predicted class: measure\n",
      "Number of predicted class: 3\n"
     ]
    }
   ],
   "source": [
    "#верно (с валидации)\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "video_path = \"/root/tatneft/datasets/violations_dataset/cuts1/ch03_20231002080315_cut_002613_002618.mp4\"\n",
    "test_video(model_path, video_path, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cf0d116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_762807/2322475451.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of Predicted class: syringing\n",
      "Number of predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "#неверно долнжо быть 0 (с валидации)\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "video_path = \"/root/tatneft/datasets/violations_dataset/cuts1/ch03_20231002080000_cut_000036_000041.mp4\"\n",
    "test_video(model_path, video_path, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9be4eb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_762807/2322475451.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of Predicted class: syringing\n",
      "Number of predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "#верно (с валидации)\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "video_path = \"/root/tatneft/datasets/violations_dataset/cuts1/ch03_20231002080315_cut_001021_001026.mp4\"\n",
    "test_video(model_path, video_path, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43dd603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_762807/2322475451.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of Predicted class: pipe_work\n",
      "Number of predicted class: 4\n"
     ]
    }
   ],
   "source": [
    "#верно (с валидации)\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "video_path = \"/root/tatneft/datasets/violations_dataset/cuts1/ch03_20231002080315_cut_004400_004405.mp4\"\n",
    "test_video(model_path, video_path, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "838fef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_762807/2322475451.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of Predicted class: HRW_work\n",
      "Number of predicted class: 5\n"
     ]
    }
   ],
   "source": [
    "#верно (с валидации)\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "video_path = \"/root/tatneft/datasets/violations_dataset/cuts1/ch03_20231002080315_cut_005031_005036.mp4\"\n",
    "test_video(model_path, video_path, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b02643bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_762807/2322475451.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of Predicted class: HRW_work\n",
      "Number of predicted class: 5\n"
     ]
    }
   ],
   "source": [
    "#неверно должен быть 7 (с валидации)\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "video_path = \"/root/tatneft/datasets/violations_dataset/cuts1/ch03_20231002080315_cut_005304_005309.mp4\"\n",
    "test_video(model_path, video_path, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ed4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Validation statistic check\"\"\"\n",
    "\n",
    "df = pd.DataFrame(columns=['video', 'prediction', 'label', 'duration'])\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "val_labels = []\n",
    "with open('/root/tatneft/datasets/violations_dataset/cuts1_val.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            video_file, label = line.strip().split()\n",
    "            val_labels.append((video_file, int(label)))\n",
    "\n",
    "for video, label in val_labels:\n",
    "    video_path = '/root/tatneft/datasets/violations_dataset/cuts1/' + video\n",
    "    predict = test_video(model_path, video_path, encoder)\n",
    "    clip = VideoFileClip(video_path)\n",
    "    new_row = {\n",
    "        'video': video,\n",
    "        'prediction': predict,\n",
    "        'label': label, \n",
    "        'duration': clip.duration\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "print(df)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tatneft",
   "language": "python",
   "name": "tatneft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06b8a0459549421395de7718536417df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "235d7ce842f247a09ae3a55ff2071f8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3907e592cb9c445c845733a557ae1720": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db1c0c82310a4e01961a90095f4f29b4",
      "placeholder": "​",
      "style": "IPY_MODEL_06b8a0459549421395de7718536417df",
      "value": " 2/465 [00:00&lt;00:04, 93.07it/s]"
     }
    },
    "4554afe822fb48a4af582d76276a7d24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6645df8386b9451baa975f7b1cdaf07d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7301625c753d4f2d906ac5810512c5ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4554afe822fb48a4af582d76276a7d24",
      "max": 465,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6645df8386b9451baa975f7b1cdaf07d",
      "value": 2
     }
    },
    "a883fb8a46844925943690fad81370bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa7b09bbf6d640e5bd5b67f943206dfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f7e1735e2f734d209e0210bddbae583b",
       "IPY_MODEL_7301625c753d4f2d906ac5810512c5ae",
       "IPY_MODEL_3907e592cb9c445c845733a557ae1720"
      ],
      "layout": "IPY_MODEL_235d7ce842f247a09ae3a55ff2071f8e"
     }
    },
    "db1c0c82310a4e01961a90095f4f29b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e734a380e7f142e6bdd5d3f8d218c0d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7e1735e2f734d209e0210bddbae583b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e734a380e7f142e6bdd5d3f8d218c0d9",
      "placeholder": "​",
      "style": "IPY_MODEL_a883fb8a46844925943690fad81370bc",
      "value": "  0%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
