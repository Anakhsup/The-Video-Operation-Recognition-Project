{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from moviepy.editor import VideoFileClip\n",
    "from tqdm.autonotebook import tqdm\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RkVdgMkEOQTJ",
   "metadata": {
    "id": "RkVdgMkEOQTJ"
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1yjaLqF5ODzm",
   "metadata": {
    "id": "1yjaLqF5ODzm"
   },
   "outputs": [],
   "source": [
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"\n",
    "    3x3x3 convolution with padding.\n",
    "\n",
    "    Args:\n",
    "        in_planes (int): Number of input channels.\n",
    "        out_planes (int): Number of output channels.\n",
    "        stride (int): Stride of the convolution.\n",
    "\n",
    "    Returns:\n",
    "        nn.Conv3d: Convolutional layer.\n",
    "    \"\"\"\n",
    "    return nn.Conv3d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=1,\n",
    "        bias=False)\n",
    "\n",
    "def downsample_basic_block(x, planes, stride):\n",
    "    \"\"\"\n",
    "    Downsample the input tensor using average pooling and zero-padding.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        planes (int): Number of output channels.\n",
    "        stride (int): Stride of the pooling.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Downsampled tensor.\n",
    "    \"\"\"\n",
    "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "    zero_pads = torch.Tensor(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4)).zero_()\n",
    "    if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "        zero_pads = zero_pads.cuda()\n",
    "    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n",
    "\n",
    "    return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Bottleneck block for ResNet.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, head_conv=1):\n",
    "        \"\"\"\n",
    "        Initializes the Bottleneck block.\n",
    "\n",
    "        Args:\n",
    "            inplanes (int): Number of input channels.\n",
    "            planes (int): Number of output channels.\n",
    "            stride (int): Stride of the convolution.\n",
    "            downsample (nn.Module, optional): Downsampling layer. Defaults to None.\n",
    "            head_conv (int, optional): Type of head convolution. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if head_conv == 1:\n",
    "            self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm3d(planes)\n",
    "        elif head_conv == 3:\n",
    "            self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=(3, 1, 1), bias=False, padding=(1, 0, 0))\n",
    "            self.bn1 = nn.BatchNorm3d(planes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported head_conv!\")\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            planes, planes, kernel_size=(1, 3, 3), stride=(1, stride, stride), padding=(0, 1, 1), bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Bottleneck block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def get_fine_tuning_parameters(model, ft_begin_index):\n",
    "    \"\"\"\n",
    "    Get the parameters for fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to fine-tune.\n",
    "        ft_begin_index (int): Index of the first layer to fine-tune.\n",
    "\n",
    "    Returns:\n",
    "        list: List of parameters.\n",
    "    \"\"\"\n",
    "    if ft_begin_index == 0:\n",
    "        return model.parameters()\n",
    "\n",
    "    ft_module_names = []\n",
    "    for i in range(ft_begin_index, 5):\n",
    "        ft_module_names.append('layer{}'.format(i))\n",
    "    ft_module_names.append('fc')\n",
    "\n",
    "    parameters = []\n",
    "    for k, v in model.named_parameters():\n",
    "        for ft_module in ft_module_names:\n",
    "            if ft_module in k:\n",
    "                parameters.append({'params': v})\n",
    "                break\n",
    "        else:\n",
    "            parameters.append({'params': v, 'lr': 0.0})\n",
    "\n",
    "    return parameters\n",
    "\n",
    "class SlowFast(nn.Module):\n",
    "    \"\"\"\n",
    "    SlowFast network for video recognition.\n",
    "    \"\"\"\n",
    "    def __init__(self, block=Bottleneck, layers=[3, 4, 6, 3], class_num=27, shortcut_type='B', dropout=0.5,\n",
    "                 alpha=8, beta=0.125):\n",
    "        \"\"\"\n",
    "        Initializes the SlowFast network.\n",
    "\n",
    "        Args:\n",
    "            block (nn.Module, optional): Bottleneck block. Defaults to Bottleneck.\n",
    "            layers (list, optional): Number of layers in each block. Defaults to [3, 4, 6, 3].\n",
    "            class_num (int, optional): Number of classes. Defaults to 27.\n",
    "            shortcut_type (str, optional): Type of shortcut connection. Defaults to 'B'.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.5.\n",
    "            alpha (int, optional): Temporal stride for slow path. Defaults to 8.\n",
    "            beta (float, optional): Channel ratio between fast and slow path. Defaults to 0.125.\n",
    "        \"\"\"\n",
    "        super(SlowFast, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        self.fast_inplanes = int(64 * beta)\n",
    "        fast_inplanes = self.fast_inplanes\n",
    "        self.fast_conv1 = nn.Conv3d(3, fast_inplanes, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3),\n",
    "                                    bias=False)\n",
    "        self.fast_bn1 = nn.BatchNorm3d(8)\n",
    "        self.fast_relu = nn.ReLU(inplace=True)\n",
    "        self.fast_maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n",
    "        self.fast_res1 = self._make_layer_fast(block, 8, layers[0], shortcut_type, head_conv=3)\n",
    "        self.fast_res2 = self._make_layer_fast(\n",
    "            block, 16, layers[1], shortcut_type, stride=2, head_conv=3)\n",
    "        self.fast_res3 = self._make_layer_fast(\n",
    "            block, 32, layers[2], shortcut_type, stride=2, head_conv=3)\n",
    "        self.fast_res4 = self._make_layer_fast(\n",
    "            block, 64, layers[3], shortcut_type, stride=2, head_conv=3)\n",
    "\n",
    "        self.slow_inplanes = 64\n",
    "        slow_inplanes = self.slow_inplanes\n",
    "        self.slow_conv1 = nn.Conv3d(3, slow_inplanes, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3),\n",
    "                                    bias=False)\n",
    "        self.slow_bn1 = nn.BatchNorm3d(64)\n",
    "        self.slow_relu = nn.ReLU(inplace=True)\n",
    "        self.slow_maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n",
    "        self.slow_res1 = self._make_layer_slow(block, 64, layers[0], shortcut_type, head_conv=1)\n",
    "        self.slow_res2 = self._make_layer_slow(\n",
    "            block, 128, layers[1], shortcut_type, stride=2, head_conv=1)\n",
    "        self.slow_res3 = self._make_layer_slow(\n",
    "            block, 256, layers[2], shortcut_type, stride=2, head_conv=1)\n",
    "        self.slow_res4 = self._make_layer_slow(\n",
    "            block, 512, layers[3], shortcut_type, stride=2, head_conv=1)\n",
    "\n",
    "        self.Tconv1 = nn.Conv3d(8, 16, kernel_size=(5, 1, 1), stride=(alpha, 1, 1), padding=(2, 0, 0), bias=False)\n",
    "        self.Tconv2 = nn.Conv3d(32, 64, kernel_size=(5, 1, 1), stride=(alpha, 1, 1), padding=(2, 0, 0), bias=False)\n",
    "        self.Tconv3 = nn.Conv3d(64, 128, kernel_size=(5, 1, 1), stride=(alpha, 1, 1), padding=(2, 0, 0), bias=False)\n",
    "        self.Tconv4 = nn.Conv3d(128, 256, kernel_size=(5, 1, 1), stride=(alpha, 1, 1), padding=(2, 0, 0), bias=False)\n",
    "\n",
    "        self.dp = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.fast_inplanes + self.slow_inplanes, class_num)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass of the SlowFast network.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        fast, Tc = self.FastPath(input[:, :, ::2, :, :])\n",
    "        slow = self.SlowPath(input[:, :, ::16, :, :], Tc)\n",
    "        x = torch.cat([slow, fast], dim=1)\n",
    "        x = self.dp(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def SlowPath(self, input, Tc):\n",
    "        \"\"\"\n",
    "        Forward pass of the slow path.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "            Tc (list): List of temporal convolutional features from the fast path.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        x = self.slow_conv1(input)\n",
    "        x = self.slow_bn1(x)\n",
    "        x = self.slow_relu(x)\n",
    "        x = self.slow_maxpool(x)\n",
    "        x = torch.cat([x, Tc[0]], dim=1)\n",
    "        x = self.slow_res1(x)\n",
    "        x = torch.cat([x, Tc[1]], dim=1)\n",
    "        x = self.slow_res2(x)\n",
    "        x = torch.cat([x, Tc[2]], dim=1)\n",
    "        x = self.slow_res3(x)\n",
    "        x = torch.cat([x, Tc[3]], dim=1)\n",
    "        x = self.slow_res4(x)\n",
    "        x = nn.AdaptiveAvgPool3d(1)(x)\n",
    "        x = x.view(-1, x.size(1))\n",
    "        return x\n",
    "\n",
    "    def FastPath(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass of the fast path.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Output tensor and list of temporal convolutional features.\n",
    "        \"\"\"\n",
    "        x = self.fast_conv1(input)\n",
    "        x = self.fast_bn1(x)\n",
    "        x = self.fast_relu(x)\n",
    "        x = self.fast_maxpool(x)\n",
    "        Tc1 = self.Tconv1(x)\n",
    "        x = self.fast_res1(x)\n",
    "        Tc2 = self.Tconv2(x)\n",
    "        x = self.fast_res2(x)\n",
    "        Tc3 = self.Tconv3(x)\n",
    "        x = self.fast_res3(x)\n",
    "        Tc4 = self.Tconv4(x)\n",
    "        x = self.fast_res4(x)\n",
    "        x = nn.AdaptiveAvgPool3d(1)(x)\n",
    "        x = x.view(-1, x.size(1))\n",
    "        return x, [Tc1, Tc2, Tc3, Tc4]\n",
    "\n",
    "    def _make_layer_fast(self, block, planes, blocks, shortcut_type, stride=1, head_conv=1):\n",
    "        \"\"\"\n",
    "        Make a fast layer.\n",
    "\n",
    "        Args:\n",
    "            block (nn.Module): Bottleneck block.\n",
    "            planes (int): Number of output channels.\n",
    "            blocks (int): Number of blocks in the layer.\n",
    "            shortcut_type (str): Type of shortcut connection.\n",
    "            stride (int, optional): Stride of the convolution. Defaults to 1.\n",
    "            head_conv (int, optional): Type of head convolution. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: Sequential layer.\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if stride != 1 or self.fast_inplanes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(\n",
    "                    downsample_basic_block,\n",
    "                    planes=planes * block.expansion,\n",
    "                    stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(\n",
    "                        self.fast_inplanes,\n",
    "                        planes * block.expansion,\n",
    "                        kernel_size=1,\n",
    "                        stride=(1, stride, stride),\n",
    "                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.fast_inplanes, planes, stride, downsample, head_conv=head_conv))\n",
    "        self.fast_inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.fast_inplanes, planes, head_conv=head_conv))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_layer_slow(self, block, planes, blocks, shortcut_type, stride=1, head_conv=1):\n",
    "        \"\"\"\n",
    "        Make a slow layer.\n",
    "\n",
    "        Args:\n",
    "            block (nn.Module): Bottleneck block.\n",
    "            planes (int): Number of output channels.\n",
    "            blocks (int): Number of blocks in the layer.\n",
    "            shortcut_type (str): Type of shortcut connection.\n",
    "            stride (int, optional): Stride of the convolution. Defaults to 1.\n",
    "            head_conv (int, optional): Type of head convolution. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: Sequential layer.\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if stride != 1 or self.slow_inplanes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(\n",
    "                    downsample_basic_block,\n",
    "                    planes=planes * block.expansion,\n",
    "                    stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(\n",
    "                        self.slow_inplanes + self.slow_inplanes // self.alpha * 2,\n",
    "                        planes * block.expansion,\n",
    "                        kernel_size=1,\n",
    "                        stride=(1, stride, stride),\n",
    "                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.slow_inplanes + self.slow_inplanes // self.alpha * 2, planes, stride, downsample,\n",
    "                            head_conv=head_conv))\n",
    "        self.slow_inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.slow_inplanes, planes, head_conv=head_conv))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-50 model.\n",
    "    \"\"\"\n",
    "    model = SlowFast(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-101 model.\n",
    "    \"\"\"\n",
    "    model = SlowFast(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-152 model.\n",
    "    \"\"\"\n",
    "    model = SlowFast(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet200(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet-200 model.\n",
    "    \"\"\"\n",
    "    model = SlowFast(Bottleneck, [3, 24, 36, 3], **kwargs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MmrPHRFPOUxy",
   "metadata": {
    "id": "MmrPHRFPOUxy"
   },
   "source": [
    "Clr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9mrP5Gt6OVsi",
   "metadata": {
    "id": "9mrP5Gt6OVsi"
   },
   "outputs": [],
   "source": [
    "class OneCycle(object):\n",
    "    \"\"\"Implements the 1cycle learning rate/momentum scheduler.\n",
    "    \n",
    "    Cycles LR between max_lr/div and max_lr, and momentum between high/low values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nb, max_lr, momentum_vals=(0.95, 0.85), prcnt=10, div=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            nb: Total number of iterations\n",
    "            max_lr: Maximum learning rate\n",
    "            momentum_vals: Tuple of (high_momentum, low_momentum)\n",
    "            prcnt: Percentage of iterations for annealing (default: 10)\n",
    "            div: Division factor for minimum LR (default: 10)\n",
    "        \"\"\"\n",
    "        self.nb = nb\n",
    "        self.div = div\n",
    "        self.step_len = int(self.nb * (1 - prcnt/100)/2)\n",
    "        self.high_lr = max_lr\n",
    "        self.low_mom = momentum_vals[1]\n",
    "        self.high_mom = momentum_vals[0]\n",
    "        self.prcnt = prcnt\n",
    "        self.iteration = 0\n",
    "        self.lrs = []\n",
    "        self.moms = []\n",
    "\n",
    "    def calc(self):\n",
    "        \"\"\"Returns current (learning_rate, momentum) pair.\"\"\"\n",
    "        self.iteration += 1\n",
    "        lr = self.calc_lr()\n",
    "        mom = self.calc_mom()\n",
    "        return (lr, mom)\n",
    "\n",
    "    def calc_lr(self):\n",
    "        \"\"\"Calculates current learning rate based on iteration progress.\"\"\"\n",
    "        if self.iteration == self.nb:\n",
    "            self.iteration = 0\n",
    "            self.lrs.append(self.high_lr/self.div)\n",
    "            return self.high_lr/self.div\n",
    "        if self.iteration > 2 * self.step_len:\n",
    "            ratio = (self.iteration - 2 * self.step_len) / (self.nb - 2 * self.step_len)\n",
    "            lr = self.high_lr * (1 - 0.99 * ratio)/self.div\n",
    "        elif self.iteration > self.step_len:\n",
    "            ratio = 1 - (self.iteration - self.step_len)/self.step_len\n",
    "            lr = self.high_lr * (1 + ratio * (self.div - 1)) / self.div\n",
    "        else:\n",
    "            ratio = self.iteration/self.step_len\n",
    "            lr = self.high_lr * (1 + ratio * (self.div - 1)) / self.div\n",
    "        self.lrs.append(lr)\n",
    "        return lr\n",
    "\n",
    "    def calc_mom(self):\n",
    "        \"\"\"Calculates current momentum based on iteration progress.\"\"\"\n",
    "        if self.iteration == self.nb:\n",
    "            self.iteration = 0\n",
    "            self.moms.append(self.high_mom)\n",
    "            return self.high_mom\n",
    "        if self.iteration > 2 * self.step_len:\n",
    "            mom = self.high_mom\n",
    "        elif self.iteration > self.step_len:\n",
    "            ratio = (self.iteration - self.step_len)/self.step_len\n",
    "            mom = self.low_mom + ratio * (self.high_mom - self.low_mom)\n",
    "        else:\n",
    "            ratio = self.iteration/self.step_len\n",
    "            mom = self.high_mom - ratio * (self.high_mom - self.low_mom)\n",
    "        self.moms.append(mom)\n",
    "        return mom\n",
    "\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    \"\"\"Updates learning rate for all parameter groups in optimizer.\"\"\"\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "\n",
    "def update_mom(optimizer, mom):\n",
    "    \"\"\"Updates momentum for all parameter groups in optimizer.\"\"\"\n",
    "    for g in optimizer.param_groups:\n",
    "        g['momentum'] = mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "r6BlG4IhKyAF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6BlG4IhKyAF",
    "outputId": "22b7eee8-5925-4447-fc7e-56fecc9295b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gas_analyzer', 'syringing', 'inspection', 'measure', 'pipe_work', 'HRW_work', 'pipe_up', 'pipe_down', 'cleaning', 'open_mouth', 'gaskets', 'spider_landing', 'unscrewing PCP', 'remove_PCP', 'unscrewing_PCP', 'crane_lowering', 'crane_lifting', 'PCP_roll', 'cross_rotation']\n",
      "Decoder: {'gas_analyzer': 0, 'syringing': 1, 'inspection': 2, 'measure': 3, 'pipe_work': 4, 'HRW_work': 5, 'pipe_up': 6, 'pipe_down': 7, 'cleaning': 8, 'open_mouth': 9, 'gaskets': 10, 'spider_landing': 11, 'unscrewing PCP': 12, 'remove_PCP': 13, 'unscrewing_PCP': 14, 'crane_lowering': 15, 'crane_lifting': 16, 'PCP_roll': 17, 'cross_rotation': 18}\n",
      "Encoder: {0: 'gas_analyzer', 1: 'syringing', 2: 'inspection', 3: 'measure', 4: 'pipe_work', 5: 'HRW_work', 6: 'pipe_up', 7: 'pipe_down', 8: 'cleaning', 9: 'open_mouth', 10: 'gaskets', 11: 'spider_landing', 12: 'unscrewing PCP', 13: 'remove_PCP', 14: 'unscrewing_PCP', 15: 'crane_lowering', 16: 'crane_lifting', 17: 'PCP_roll', 18: 'cross_rotation'}\n"
     ]
    }
   ],
   "source": [
    "data_path = '/root/tatneft/datasets/violations_dataset/cuts1_labels.txt'\n",
    "\n",
    "# Read class labels from file and store them in a list\n",
    "with open(data_path, 'r') as file:\n",
    "    classes = [line.strip() for line in file.readlines()]\n",
    "\n",
    "print(classes)\n",
    "\n",
    "# Create a decoder dictionary that maps class labels to their index\n",
    "decoder = {classes[i]: i for i in range(len(classes))}\n",
    "\n",
    "# Create an encoder dictionary that maps indices to class labels\n",
    "encoder = {i: classes[i] for i in range(len(classes))}\n",
    "\n",
    "print(\"Decoder:\", decoder)\n",
    "print(\"Encoder:\", encoder)\n",
    "\n",
    "# List to store video paths and class labels\n",
    "id_list = []\n",
    "\n",
    "# Path to the annotation file containing video paths and corresponding class labels\n",
    "annotations_path = '/root/tatneft/datasets/violations_dataset/cuts1_train.txt'\n",
    "\n",
    "# Read the annotation file and store video paths with class labels in id_list\n",
    "with open(annotations_path, 'r') as file:\n",
    "    for line in file:\n",
    "        video_path, class_label = line.strip().split()\n",
    "        id_list.append((video_path, int(class_label)))\n",
    "\n",
    "# Print the total number of entries and the first five samples\n",
    "print(len(id_list))\n",
    "print(id_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89b1b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class video_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading video sequences with fixed frame length.\n",
    "    \"\"\"\n",
    "    def __init__(self, frame_list, sequence_length=16, transform=None, im_size=256):\n",
    "        self.frame_list = frame_list\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "        self.im_size = im_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.frame_list[idx]\n",
    "        path = '/root/tatneft/datasets/violations_dataset/cuts1/' + path\n",
    "\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Failed to open video file: {path}\")\n",
    "\n",
    "        frames = []\n",
    "        while len(frames) < self.sequence_length:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        while len(frames) < self.sequence_length:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "\n",
    "        seq_image = torch.stack(frames)  # Shape: (sequence_length, channels, height, width)\n",
    "        seq_image = seq_image.permute(1, 0, 2, 3)  # Shape: (channels, sequence_length, height, width)\n",
    "\n",
    "        return seq_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f436a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6f436a2",
    "outputId": "4c023790-c064-4e80-ba8c-155aac88b86a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'val'])\n"
     ]
    }
   ],
   "source": [
    "# Image parameters\n",
    "im_size = 128\n",
    "mean = [0.4889, 0.4887, 0.4891]\n",
    "std = [0.2074, 0.2074, 0.2074]\n",
    "\n",
    "# Define transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((im_size, im_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load training data\n",
    "train_data = video_dataset(id, sequence_length=10, transform=train_transforms)\n",
    "train_loader = DataLoader(train_data, batch_size=8, num_workers=1, shuffle=True)\n",
    "\n",
    "# Load validation data\n",
    "val_path = '/root/tatneft/datasets/violations_dataset/cuts1_val.txt'\n",
    "val_id = []\n",
    "with open(val_path, 'r') as file:\n",
    "    for line in file:\n",
    "        video_path, class_label = line.strip().split()\n",
    "        val_id.append((video_path, int(class_label)))\n",
    "\n",
    "val_data = video_dataset(val_id, sequence_length=10, transform=train_transforms)\n",
    "val_loader = DataLoader(val_data, batch_size=8, num_workers=1, shuffle=False)\n",
    "\n",
    "# Create dataloaders dictionary\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "# Print available keys\n",
    "print(dataloaders.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db7609be",
   "metadata": {
    "id": "db7609be"
   },
   "outputs": [],
   "source": [
    "model = resnet200(class_num=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faada049",
   "metadata": {
    "id": "faada049"
   },
   "outputs": [],
   "source": [
    "\"\"\"Learnig parameters\"\"\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "cls_criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum = 0.9,weight_decay = 1e-4)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "onecyc = OneCycle(len(train_loader)*num_epochs,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c25137be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:43,  2.15s/it]\n",
      "10it [00:20,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.6855, Val_loss: 65.3912, Precision: 0.1157, Recall: 0.2785, F1: 0.1612\n",
      "\n",
      "--- Epoch 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:37,  2.04s/it]\n",
      "10it [00:19,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.6410, Val_loss: 3.8591, Precision: 0.1792, Recall: 0.2278, F1: 0.1901\n",
      "\n",
      "--- Epoch 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:36,  2.02s/it]\n",
      "10it [00:19,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.6023, Val_loss: 2.4220, Precision: 0.1760, Recall: 0.3038, F1: 0.2116\n",
      "\n",
      "--- Epoch 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:39,  2.07s/it]\n",
      "10it [00:20,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.6003, Val_loss: 2.1498, Precision: 0.2019, Recall: 0.2911, F1: 0.2326\n",
      "\n",
      "--- Epoch 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:36,  2.02s/it]\n",
      "10it [00:19,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.5035, Val_loss: 2.0964, Precision: 0.2764, Recall: 0.4557, F1: 0.3300\n",
      "\n",
      "--- Epoch 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:39,  2.07s/it]\n",
      "10it [00:19,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.4698, Val_loss: 2.0740, Precision: 0.2466, Recall: 0.3418, F1: 0.2723\n",
      "\n",
      "--- Epoch 7 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:54,  2.38s/it]\n",
      "10it [00:20,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.4280, Val_loss: 1.9892, Precision: 0.2316, Recall: 0.3038, F1: 0.2426\n",
      "\n",
      "--- Epoch 8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:37,  2.03s/it]\n",
      "10it [00:20,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.4173, Val_loss: 3.6172, Precision: 0.1321, Recall: 0.2785, F1: 0.1739\n",
      "\n",
      "--- Epoch 9 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:39,  2.08s/it]\n",
      "10it [00:20,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.4288, Val_loss: 2.0961, Precision: 0.2880, Recall: 0.4430, F1: 0.3292\n",
      "\n",
      "--- Epoch 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:37,  2.03s/it]\n",
      "10it [00:20,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.1298, Val_loss: 1.9297, Precision: 0.3571, Recall: 0.3165, F1: 0.2943\n",
      "\n",
      "--- Epoch 11 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:36,  2.02s/it]\n",
      "10it [00:19,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.1147, Val_loss: 1.7433, Precision: 0.3899, Recall: 0.4304, F1: 0.3913\n",
      "\n",
      "--- Epoch 12 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:38,  2.05s/it]\n",
      "10it [00:20,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.0902, Val_loss: 1.7476, Precision: 0.4574, Recall: 0.3544, F1: 0.3323\n",
      "\n",
      "--- Epoch 13 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:37,  2.03s/it]\n",
      "10it [00:19,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.0066, Val_loss: 1.4516, Precision: 0.3391, Recall: 0.4177, F1: 0.3637\n",
      "\n",
      "--- Epoch 14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:40,  2.09s/it]\n",
      "10it [00:20,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.7505, Val_loss: 1.6653, Precision: 0.3179, Recall: 0.3671, F1: 0.3296\n",
      "\n",
      "--- Epoch 15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:38,  2.05s/it]\n",
      "10it [00:20,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.8837, Val_loss: 1.4327, Precision: 0.4706, Recall: 0.5443, F1: 0.4846\n",
      "\n",
      "--- Epoch 16 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:38,  2.05s/it]\n",
      "10it [00:20,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.7939, Val_loss: 1.5954, Precision: 0.3680, Recall: 0.4177, F1: 0.3639\n",
      "\n",
      "--- Epoch 17 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:37,  2.02s/it]\n",
      "10it [00:20,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.7613, Val_loss: 1.1609, Precision: 0.3662, Recall: 0.5949, F1: 0.4510\n",
      "\n",
      "--- Epoch 18 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:37,  2.03s/it]\n",
      "10it [00:19,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.4146, Val_loss: 1.3356, Precision: 0.4345, Recall: 0.4557, F1: 0.3942\n",
      "\n",
      "--- Epoch 19 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [01:34,  1.97s/it]\n",
      "10it [00:19,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.5187, Val_loss: 1.0230, Precision: 0.5884, Recall: 0.6962, F1: 0.6200\n",
      "\n",
      "--- Epoch 20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:35,  2.00s/it]\n",
      "10it [00:19,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.3856, Val_loss: 1.0356, Precision: 0.5970, Recall: 0.6709, F1: 0.5832\n",
      "Best F1-score: 0.6200 in epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create directory for saving model weights\n",
    "os.makedirs('/root/akhsup/weights', exist_ok=True)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Initialize training variables\n",
    "iteration = 0\n",
    "train_losses, val_losses = [], []\n",
    "precisions, recalls, f1_scores = [], [], []\n",
    "train_loss, val_loss = 0.0, 0.0\n",
    "best_f1 = 0.0\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n--- Epoch {epoch+1} ---\")\n",
    "    \n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        all_labels, all_preds = [], []\n",
    "\n",
    "        for batch_i, (X, y) in tqdm(enumerate(dataloaders[phase])):\n",
    "            image_sequences = X.to(device)\n",
    "            labels = y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                predictions = model(image_sequences)\n",
    "                loss = cls_criterion(predictions, labels)\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * image_sequences.size(0)\n",
    "            _, preds = torch.max(predictions, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            # Update learning rate and momentum for training phase\n",
    "            if phase == 'train':\n",
    "                lr, mom = onecyc.calc()\n",
    "                update_lr(optimizer, lr)\n",
    "                update_mom(optimizer, mom)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Compute average loss and metrics\n",
    "        epoch_loss /= len(dataloaders[phase].dataset)\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        \n",
    "        if phase == 'train':\n",
    "            train_losses.append(epoch_loss)\n",
    "            train_loss = epoch_loss\n",
    "        else:\n",
    "            val_losses.append(epoch_loss)\n",
    "            val_loss = epoch_loss\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "            # Save best model based on F1-score\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), f'/root/akhsup/weights/best_model_f1_{best_epoch}.pth')\n",
    "    \n",
    "    print(f\"Train_loss: {train_loss:.4f}, Val_loss: {val_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "print(f\"Best F1-score: {best_f1:.4f} in epoch {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "526204b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss (try 1)')\n",
    "plt.legend()\n",
    "plt.savefig('/root/akhsup/weights/loss_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ae790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, num_classes=19):\n",
    "    \"\"\"\n",
    "    Load the pretrained model from the specified path.\n",
    "    \"\"\"\n",
    "    model = resnet200(class_num=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_video(video_path, sequence_length=10, im_size=128):\n",
    "    \"\"\"\n",
    "    Preprocess the video into a tensor that can be fed into the model.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((im_size, im_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4889, 0.4887, 0.4891], std=[0.2074, 0.2074, 0.2074])\n",
    "    ])\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while len(frames) < sequence_length:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = transform(frame) \n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        raise ValueError(f\":  '{video_path}'      .\")\n",
    "\n",
    "    # Pad the frames to ensure the sequence length is met\n",
    "    while len(frames) < sequence_length:\n",
    "        frames.append(torch.zeros_like(frames[-1]))\n",
    "\n",
    "    # Convert frames to a tensor and permute to (channels, sequence_length, height, width)\n",
    "    video_tensor = torch.stack(frames)  # (sequence_length, channels, height, width)\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)  # (channels, sequence_length, height, width)\n",
    "    \n",
    "    return video_tensor.unsqueeze(0)  # Add batch dimension: (1, channels, sequence_length, height, width)\n",
    "\n",
    "def predict_video(model, video_tensor, device):\n",
    "    \"\"\"\n",
    "    Perform prediction on a video tensor using the given model.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        video_tensor = video_tensor.to(device)\n",
    "        outputs = model(video_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "def test_video(model_path, video_path, encoder, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Test the model on a video, returning the predicted class.\n",
    "    \"\"\"\n",
    "    model = load_model(model_path)\n",
    "    model = model.to(device)\n",
    "\n",
    "    video_tensor = preprocess_video(video_path)\n",
    "\n",
    "    predicted_class = predict_video(model, video_tensor, device)\n",
    "    \n",
    "    # Map the predicted class index to its label\n",
    "    predicted_label = encoder.get(predicted_class, \"Unknown class\")\n",
    "    \n",
    "    # You can print the label or the class index for debugging or logging\n",
    "    print(f\"Predicted class: {predicted_label} (index {predicted_class})\")\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     model_path = \"/path/to/model.pth\"\n",
    "#     video_path = \"/path/to/video.mp4\"\n",
    "#     encoder = {0: \"Class1\", 1: \"Class2\", 2: \"Class3\", ...}  # Define your encoder\n",
    "#     predicted_class = test_video(model_path, video_path, encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d05a3e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_762807/2322475451.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of Predicted class: measure\n",
      "Number of predicted class: 3\n"
     ]
    }
   ],
   "source": [
    "# ( )\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "video_path = \"/root/tatneft/datasets/violations_dataset/cuts1/ch03_20231002080315_cut_002613_002618.mp4\"\n",
    "test_video(model_path, video_path, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cf0d116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_762807/2322475451.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of Predicted class: syringing\n",
      "Number of predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "#   0 ( )\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "video_path = \"/root/tatneft/datasets/violations_dataset/cuts1/ch03_20231002080000_cut_000036_000041.mp4\"\n",
    "test_video(model_path, video_path, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9be4eb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_762807/2322475451.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of Predicted class: syringing\n",
      "Number of predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "# ( )\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "video_path = \"/root/tatneft/datasets/violations_dataset/cuts1/ch03_20231002080315_cut_001021_001026.mp4\"\n",
    "test_video(model_path, video_path, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43dd603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_762807/2322475451.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of Predicted class: pipe_work\n",
      "Number of predicted class: 4\n"
     ]
    }
   ],
   "source": [
    "# ( )\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "video_path = \"/root/tatneft/datasets/violations_dataset/cuts1/ch03_20231002080315_cut_004400_004405.mp4\"\n",
    "test_video(model_path, video_path, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "838fef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_762807/2322475451.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of Predicted class: HRW_work\n",
      "Number of predicted class: 5\n"
     ]
    }
   ],
   "source": [
    "# ( )\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "video_path = \"/root/tatneft/datasets/violations_dataset/cuts1/ch03_20231002080315_cut_005031_005036.mp4\"\n",
    "test_video(model_path, video_path, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b02643bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_762807/2322475451.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of Predicted class: HRW_work\n",
      "Number of predicted class: 5\n"
     ]
    }
   ],
   "source": [
    "#   7 ( )\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "video_path = \"/root/tatneft/datasets/violations_dataset/cuts1/ch03_20231002080315_cut_005304_005309.mp4\"\n",
    "test_video(model_path, video_path, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ed4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Validation statistic check\"\"\"\n",
    "\n",
    "df = pd.DataFrame(columns=['video', 'prediction', 'label', 'duration'])\n",
    "model_path = \"/root/akhsup/weights/best_model_f1_19.pth\"\n",
    "val_labels = []\n",
    "with open('/root/tatneft/datasets/violations_dataset/cuts1_val.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            video_file, label = line.strip().split()\n",
    "            val_labels.append((video_file, int(label)))\n",
    "\n",
    "for video, label in val_labels:\n",
    "    video_path = '/root/tatneft/datasets/violations_dataset/cuts1/' + video\n",
    "    predict = test_video(model_path, video_path, encoder)\n",
    "    clip = VideoFileClip(video_path)\n",
    "    new_row = {\n",
    "        'video': video,\n",
    "        'prediction': predict,\n",
    "        'label': label, \n",
    "        'duration': clip.duration\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "print(df)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tatneft",
   "language": "python",
   "name": "tatneft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06b8a0459549421395de7718536417df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "235d7ce842f247a09ae3a55ff2071f8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3907e592cb9c445c845733a557ae1720": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db1c0c82310a4e01961a90095f4f29b4",
      "placeholder": "",
      "style": "IPY_MODEL_06b8a0459549421395de7718536417df",
      "value": "2/465[00:00&lt;00:04,93.07it/s]"
     }
    },
    "4554afe822fb48a4af582d76276a7d24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6645df8386b9451baa975f7b1cdaf07d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7301625c753d4f2d906ac5810512c5ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4554afe822fb48a4af582d76276a7d24",
      "max": 465,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6645df8386b9451baa975f7b1cdaf07d",
      "value": 2
     }
    },
    "a883fb8a46844925943690fad81370bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa7b09bbf6d640e5bd5b67f943206dfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f7e1735e2f734d209e0210bddbae583b",
       "IPY_MODEL_7301625c753d4f2d906ac5810512c5ae",
       "IPY_MODEL_3907e592cb9c445c845733a557ae1720"
      ],
      "layout": "IPY_MODEL_235d7ce842f247a09ae3a55ff2071f8e"
     }
    },
    "db1c0c82310a4e01961a90095f4f29b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e734a380e7f142e6bdd5d3f8d218c0d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7e1735e2f734d209e0210bddbae583b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e734a380e7f142e6bdd5d3f8d218c0d9",
      "placeholder": "",
      "style": "IPY_MODEL_a883fb8a46844925943690fad81370bc",
      "value": "0%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
