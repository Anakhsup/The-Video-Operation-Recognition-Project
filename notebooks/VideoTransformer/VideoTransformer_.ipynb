{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ad06756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoImageProcessor, TimesformerForVideoClassification\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76cc4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO_DIR = '/root/tatneft/datasets/violations_dataset/cuts1'\n",
    "# LABELS_FILE = '/root/tatneft/datasets/violations_dataset/cuts1_train.txt'\n",
    "# VAL_LABELS_FILE = '/root/tatneft/datasets/violations_dataset/cuts1_val.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db1fad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIR = '/root/tatneft/datasets/violations_dataset/cuts1'\n",
    "LABELS_FILE = '/root/tatneft/datasets/violations_dataset/cuts1_train.txt'\n",
    "VAL_LABELS_FILE = '/root/tatneft/datasets/violations_dataset/cuts1_val.txt'\n",
    "NUM_CLASSES = 12\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 2e-5\n",
    "FRAME_COUNT = 8\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7291f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tatneft/lib/python3.10/site-packages/transformers/modeling_utils.py:454: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimesformerForVideoClassification(\n",
       "  (timesformer): TimesformerModel(\n",
       "    (embeddings): TimesformerEmbeddings(\n",
       "      (patch_embeddings): TimesformerPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (time_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): TimesformerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x TimesformerLayer(\n",
       "          (drop_path): Identity()\n",
       "          (attention): TimeSformerAttention(\n",
       "            (attention): TimesformerSelfAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TimesformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TimesformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TimesformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attention): TimeSformerAttention(\n",
       "            (attention): TimesformerSelfAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TimesformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=19, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"facebook/timesformer-base-finetuned-k400\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = TimesformerForVideoClassification.from_pretrained(model_name)\n",
    "model.classifier = nn.Linear(model.classifier.in_features, NUM_CLASSES)  # Изменяем последний слой под наше число классов\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "996b68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_file):\n",
    "    \"\"\"\n",
    "    Load video file paths and corresponding labels from a text file.\n",
    "    \n",
    "    Args:\n",
    "        label_file (str): Path to the file containing video paths and labels\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tuples containing (video_path, label)\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                labels.append((parts[0], int(parts[1])))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0617060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "        \"\"\"\n",
    "        Custom PyTorch Dataset class for loading and processing video data.\n",
    "    \n",
    "        Args:\n",
    "            video_dir (str): Directory containing video files\n",
    "            label_file (str): File containing video labels\n",
    "            frame_count (int): Number of frames to extract from each video\n",
    "        \"\"\"\n",
    "    def __init__(self, video_dir, label_file, frame_count=8):\n",
    "        self.video_dir = video_dir\n",
    "        self.labels = load_labels(label_file)\n",
    "        self.frame_count = frame_count\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and process a single video sample.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to load\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (processed_frames, label) where:\n",
    "                - processed_frames: Tensor of shape (num_frames, channels, height, width)\n",
    "                - label: Integer class label\n",
    "        \"\"\"\n",
    "        video_path, label = self.labels[idx]\n",
    "        video_path = os.path.join(self.video_dir, video_path)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "\n",
    "        if not cap.isOpened(): # Handle case where video cannot be opened\n",
    "            print(f\"Ошибка: Не удалось открыть видео {video_path}\")\n",
    "            return self.__getitem__((idx + 1) % len(self.labels))\n",
    "         \n",
    "        # Calculate frame sampling step\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        step = max(1, total_frames // self.frame_count) if total_frames > 0 else 1\n",
    "\n",
    "        for i in range(self.frame_count):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (224, 224))  # Приводим к нужному размеру\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        # Handle case where no frames were read\n",
    "        if len(frames) == 0:\n",
    "            print(f\"Ошибка: Видео {video_path} не содержит кадров\")\n",
    "            return self.__getitem__((idx + 1) % len(self.labels))\n",
    "\n",
    "        # Pad with last frame if we didn't get enough frames\n",
    "        while len(frames) < self.frame_count:\n",
    "            frames.append(frames[-1])\n",
    "\n",
    "        # Преобразуем кадры в формат, ожидаемый моделью (num_frames, channels, height, width)\n",
    "        frames = np.array(frames)  # (num_frames, height, width, channels)\n",
    "        frames = np.transpose(frames, (0, 3, 1, 2))  # (num_frames, channels, height, width)\n",
    "        frames = torch.tensor(frames, dtype=torch.float32) / 255.0  # Нормализуем\n",
    "\n",
    "        return frames, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7ea62c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to properly batch samples.\n",
    "    \n",
    "    Args:\n",
    "        batch (list): List of samples from the dataset\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (batched_frames, batched_labels) where:\n",
    "            - batched_frames: Tensor of shape (batch_size, num_frames, channels, height, width)\n",
    "            - batched_labels: Tensor of shape (batch_size,)\n",
    "    \"\"\"\n",
    "    # Собираем батч: (frames, labels)\n",
    "    frames, labels = zip(*batch)\n",
    "    \n",
    "    # Преобразуем список тензоров (num_frames, channels, height, width) в один тензор\n",
    "    frames = torch.stack(frames)  # (batch_size, num_frames, channels, height, width)\n",
    "    \n",
    "    # Меняем порядок осей для модели: (batch_size, num_frames, channels, height, width)\n",
    "    # Это формат, который ожидает TimeSformer\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return frames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8eff3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(VIDEO_DIR, LABELS_FILE, frame_count=FRAME_COUNT)\n",
    "val_dataset = VideoDataset(VIDEO_DIR, VAL_LABELS_FILE, frame_count=FRAME_COUNT)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Оптимизатор и функция потерь\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72cec32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with per-class metrics\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        dataloader: DataLoader for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (val_loss, overall_metrics, class_metrics) where:\n",
    "            - val_loss: average validation loss\n",
    "            - overall_metrics: dictionary with overall metrics\n",
    "            - class_metrics: dictionary with metrics per class\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for videos, labels in dataloader:\n",
    "            videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(videos).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_metrics = {\n",
    "        'loss': val_loss / len(dataloader),\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    class_metrics = {}\n",
    "    for class_id in range(NUM_CLASSES):\n",
    "        # Create binary labels for this class\n",
    "        binary_labels = (np.array(all_labels) == class_id).astype(int)\n",
    "        binary_preds = (np.array(all_preds) == class_id).astype(int)\n",
    "        \n",
    "        # Skip if no true samples for this class\n",
    "        if sum(binary_labels) == 0:\n",
    "            continue\n",
    "            \n",
    "        class_metrics[class_id] = {\n",
    "            'precision': precision_score(binary_labels, binary_preds, zero_division=0),\n",
    "            'recall': recall_score(binary_labels, binary_preds, zero_division=0),\n",
    "            'f1': f1_score(binary_labels, binary_preds, zero_division=0),\n",
    "            'support': sum(binary_labels)  # Number of actual samples\n",
    "        }\n",
    "    \n",
    "    return val_loss / len(dataloader), overall_metrics, class_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46af664b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [1/20]\n",
      "  ▸ Потери на обучении: 1.9036\n",
      "  ▸ Потери на валидации: 1.4612\n",
      "  ▸ Accuracy: 0.6203\n",
      "  ▸ Precision: 0.5163\n",
      "  ▸ Recall: 0.6203\n",
      "  ▸ F1-score: 0.5469\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [2/20]\n",
      "  ▸ Потери на обучении: 0.9573\n",
      "  ▸ Потери на валидации: 0.7944\n",
      "  ▸ Accuracy: 0.7722\n",
      "  ▸ Precision: 0.7401\n",
      "  ▸ Recall: 0.7722\n",
      "  ▸ F1-score: 0.7493\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [3/20]\n",
      "  ▸ Потери на обучении: 0.4763\n",
      "  ▸ Потери на валидации: 0.4530\n",
      "  ▸ Accuracy: 0.8608\n",
      "  ▸ Precision: 0.7862\n",
      "  ▸ Recall: 0.8608\n",
      "  ▸ F1-score: 0.8161\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [4/20]\n",
      "  ▸ Потери на обучении: 0.3176\n",
      "  ▸ Потери на валидации: 0.4455\n",
      "  ▸ Accuracy: 0.8608\n",
      "  ▸ Precision: 0.8893\n",
      "  ▸ Recall: 0.8608\n",
      "  ▸ F1-score: 0.8644\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [5/20]\n",
      "  ▸ Потери на обучении: 0.2163\n",
      "  ▸ Потери на валидации: 0.2765\n",
      "  ▸ Accuracy: 0.9114\n",
      "  ▸ Precision: 0.8599\n",
      "  ▸ Recall: 0.9114\n",
      "  ▸ F1-score: 0.8810\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [6/20]\n",
      "  ▸ Потери на обучении: 0.1593\n",
      "  ▸ Потери на валидации: 0.2811\n",
      "  ▸ Accuracy: 0.8987\n",
      "  ▸ Precision: 0.8842\n",
      "  ▸ Recall: 0.8987\n",
      "  ▸ F1-score: 0.8845\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [7/20]\n",
      "  ▸ Потери на обучении: 0.1243\n",
      "  ▸ Потери на валидации: 0.2612\n",
      "  ▸ Accuracy: 0.8861\n",
      "  ▸ Precision: 0.9170\n",
      "  ▸ Recall: 0.8861\n",
      "  ▸ F1-score: 0.8898\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [8/20]\n",
      "  ▸ Потери на обучении: 0.0979\n",
      "  ▸ Потери на валидации: 0.2358\n",
      "  ▸ Accuracy: 0.9367\n",
      "  ▸ Precision: 0.9413\n",
      "  ▸ Recall: 0.9367\n",
      "  ▸ F1-score: 0.9343\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [9/20]\n",
      "  ▸ Потери на обучении: 0.0929\n",
      "  ▸ Потери на валидации: 0.2669\n",
      "  ▸ Accuracy: 0.8734\n",
      "  ▸ Precision: 0.9010\n",
      "  ▸ Recall: 0.8734\n",
      "  ▸ F1-score: 0.8797\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [10/20]\n",
      "  ▸ Потери на обучении: 0.0789\n",
      "  ▸ Потери на валидации: 0.2772\n",
      "  ▸ Accuracy: 0.8987\n",
      "  ▸ Precision: 0.9249\n",
      "  ▸ Recall: 0.8987\n",
      "  ▸ F1-score: 0.9023\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [11/20]\n",
      "  ▸ Потери на обучении: 0.0603\n",
      "  ▸ Потери на валидации: 0.2028\n",
      "  ▸ Accuracy: 0.9367\n",
      "  ▸ Precision: 0.9428\n",
      "  ▸ Recall: 0.9367\n",
      "  ▸ F1-score: 0.9286\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [12/20]\n",
      "  ▸ Потери на обучении: 0.0630\n",
      "  ▸ Потери на валидации: 0.2535\n",
      "  ▸ Accuracy: 0.9241\n",
      "  ▸ Precision: 0.9281\n",
      "  ▸ Recall: 0.9241\n",
      "  ▸ F1-score: 0.9176\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [13/20]\n",
      "  ▸ Потери на обучении: 0.0562\n",
      "  ▸ Потери на валидации: 0.2157\n",
      "  ▸ Accuracy: 0.9241\n",
      "  ▸ Precision: 0.9210\n",
      "  ▸ Recall: 0.9241\n",
      "  ▸ F1-score: 0.9142\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [14/20]\n",
      "  ▸ Потери на обучении: 0.0420\n",
      "  ▸ Потери на валидации: 0.3005\n",
      "  ▸ Accuracy: 0.8987\n",
      "  ▸ Precision: 0.8474\n",
      "  ▸ Recall: 0.8987\n",
      "  ▸ F1-score: 0.8668\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [15/20]\n",
      "  ▸ Потери на обучении: 0.1048\n",
      "  ▸ Потери на валидации: 0.3785\n",
      "  ▸ Accuracy: 0.8734\n",
      "  ▸ Precision: 0.8510\n",
      "  ▸ Recall: 0.8734\n",
      "  ▸ F1-score: 0.8586\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [16/20]\n",
      "  ▸ Потери на обучении: 0.0614\n",
      "  ▸ Потери на валидации: 0.2707\n",
      "  ▸ Accuracy: 0.9114\n",
      "  ▸ Precision: 0.9105\n",
      "  ▸ Recall: 0.9114\n",
      "  ▸ F1-score: 0.9032\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [17/20]\n",
      "  ▸ Потери на обучении: 0.0390\n",
      "  ▸ Потери на валидации: 0.1942\n",
      "  ▸ Accuracy: 0.9241\n",
      "  ▸ Precision: 0.9281\n",
      "  ▸ Recall: 0.9241\n",
      "  ▸ F1-score: 0.9176\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [18/20]\n",
      "  ▸ Потери на обучении: 0.0371\n",
      "  ▸ Потери на валидации: 0.3255\n",
      "  ▸ Accuracy: 0.8734\n",
      "  ▸ Precision: 0.9199\n",
      "  ▸ Recall: 0.8734\n",
      "  ▸ F1-score: 0.8842\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [19/20]\n",
      "  ▸ Потери на обучении: 0.0205\n",
      "  ▸ Потери на валидации: 0.2003\n",
      "  ▸ Accuracy: 0.9367\n",
      "  ▸ Precision: 0.9393\n",
      "  ▸ Recall: 0.9367\n",
      "  ▸ F1-score: 0.9301\n",
      "--------------------------------------------------\n",
      "пошло\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло20\n",
      "пошло!!!\n",
      "Эпоха [20/20]\n",
      "  ▸ Потери на обучении: 0.0276\n",
      "  ▸ Потери на валидации: 0.2401\n",
      "  ▸ Accuracy: 0.8987\n",
      "  ▸ Precision: 0.9395\n",
      "  ▸ Recall: 0.8987\n",
      "  ▸ F1-score: 0.9050\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for videos, labels in train_loader:\n",
    "        # ... (keep existing training code the same)\n",
    "    \n",
    "    # Evaluation phase\n",
    "    val_loss, overall_metrics, class_metrics = evaluate(model, val_loader)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "    print(f\"  Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    print(\"\\nOverall Validation Metrics:\")\n",
    "    print(f\"  Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {overall_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {overall_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {overall_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-score: {overall_metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-Class Validation Metrics:\")\n",
    "    for class_id, metrics in class_metrics.items():\n",
    "        print(f\"  Class {class_id}:\")\n",
    "        print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"    Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"    F1-score: {metrics['f1']:.4f}\")\n",
    "        print(f\"    Support: {metrics['support']}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Save model checkpoints\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        torch.save(model.state_dict(), f\"timesformer_epoch_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47725c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"timesformer_finetuned.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf570ec",
   "metadata": {},
   "source": [
    "#inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6a8f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from transformers import TimesformerForVideoClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d145ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"timesformer_epoch_20.pth\"  # Путь к сохраненной модели\n",
    "VIDEO_DIR = \"/path/to/your/videos\"  # Директория с видео для inference\n",
    "NUM_CLASSES = 19  # Должно совпадать с обучением\n",
    "FRAME_COUNT = 8  # Должно совпадать с обучением\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2bea026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tatneft/lib/python3.10/site-packages/transformers/modeling_utils.py:454: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([19, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1763916/1716605317.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimesformerForVideoClassification(\n",
       "  (timesformer): TimesformerModel(\n",
       "    (embeddings): TimesformerEmbeddings(\n",
       "      (patch_embeddings): TimesformerPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (time_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): TimesformerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x TimesformerLayer(\n",
       "          (drop_path): Identity()\n",
       "          (attention): TimeSformerAttention(\n",
       "            (attention): TimesformerSelfAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TimesformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TimesformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TimesformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attention): TimeSformerAttention(\n",
       "            (attention): TimesformerSelfAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TimesformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=19, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TimesformerForVideoClassification.from_pretrained(\n",
    "    \"facebook/timesformer-base-finetuned-k400\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30abf30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceVideoDataset(Dataset):\n",
    "\n",
    "    def __init__(self, video_paths, frame_count=8):\n",
    "        self.video_paths = video_paths\n",
    "        self.frame_count = frame_count\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        step = max(1, total_frames // self.frame_count)\n",
    "\n",
    "        for i in range(self.frame_count):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (224, 224))\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        # Если не хватает кадров, дублируем последний\n",
    "        while len(frames) < self.frame_count:\n",
    "            frames.append(frames[-1])\n",
    "\n",
    "        # Преобразуем в формат для модели\n",
    "        frames = np.array(frames)  # (num_frames, height, width, channels)\n",
    "        frames = np.transpose(frames, (0, 3, 1, 2))  # (num_frames, channels, height, width)\n",
    "        frames = torch.tensor(frames, dtype=torch.float32) / 255.0\n",
    "\n",
    "        return frames, os.path.basename(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ba1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_video(model, video_path):\n",
    "    \"\"\"\n",
    "    Make prediction for a single video file.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        video_path: Path to video file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing:\n",
    "            - filename: Name of the video file\n",
    "            - predicted_class: Predicted class index\n",
    "            - confidence: Confidence score for prediction\n",
    "            - probabilities: Array of probabilities for all classes\n",
    "    \"\"\"\n",
    "    \"\"\"Предсказание для одного видео\"\"\"\n",
    "    dataset = InferenceVideoDataset([video_path], frame_count=FRAME_COUNT)\n",
    "    frames, filename = dataset[0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = frames.unsqueeze(0).to(DEVICE)  # Добавляем batch dimension\n",
    "        outputs = model(inputs).logits\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        pred_class = torch.argmax(probs).item()\n",
    "        confidence = torch.max(probs).item()\n",
    "    \n",
    "    return {\n",
    "        \"filename\": filename,\n",
    "        \"predicted_class\": pred_class,\n",
    "        \"confidence\": confidence,\n",
    "        \"probabilities\": probs.cpu().numpy()[0]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d011a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, video_dir):\n",
    "    \"\"\"\n",
    "    Make predictions for all video files in a directory.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        video_dir: Directory containing video files\n",
    "        \n",
    "    Returns:\n",
    "        list: List of prediction dictionaries (same format as predict_single_video)\n",
    "    \"\"\"\n",
    "    \"\"\"Пакетное предсказание для всех видео в директории\"\"\"\n",
    "    video_paths = [os.path.join(video_dir, f) for f in os.listdir(video_dir) \n",
    "                  if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    dataset = InferenceVideoDataset(video_paths, frame_count=FRAME_COUNT)\n",
    "    \n",
    "    results = []\n",
    "    for frames, filename in dataset:\n",
    "        with torch.no_grad():\n",
    "            inputs = frames.unsqueeze(0).to(DEVICE)\n",
    "            outputs = model(inputs).logits\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            pred_class = torch.argmax(probs).item()\n",
    "            confidence = torch.max(probs).item()\n",
    "        \n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"predicted_class\": pred_class,\n",
    "            \"confidence\": confidence,\n",
    "            \"probabilities\": probs.cpu().numpy()[0]\n",
    "        })\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Вариант 1: Предсказание для одного видео\n",
    "video_path = \"/path/to/single/video.mp4\"\n",
    "result = predict_single_video(model, video_path)\n",
    "print(f\"Результат для {result['filename']}:\")\n",
    "print(f\"Класс: {result['predicted_class']}, Уверенность: {result['confidence']:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вариант 2: Пакетная обработка всех видео в директории\n",
    "all_results = predict_batch(model, VIDEO_DIR)\n",
    "for res in all_results:\n",
    "    print(f\"{res['filename']}: класс {res['predicted_class']} (уверенность: {res['confidence']:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tatneft",
   "language": "python",
   "name": "tatneft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
